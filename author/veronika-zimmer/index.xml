<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Veronika Zimmer | AI in Medicine</title>
    <link>https://aim-lab.io/author/veronika-zimmer/</link>
      <atom:link href="https://aim-lab.io/author/veronika-zimmer/index.xml" rel="self" type="application/rss+xml" />
    <description>Veronika Zimmer</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Technical University of Munich 2021</copyright><lastBuildDate>Sat, 14 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://aim-lab.io/images/icon_hu90763c276d9f69c3ad22e431a6bb6670_11797_512x512_fill_lanczos_center_2.png</url>
      <title>Veronika Zimmer</title>
      <link>https://aim-lab.io/author/veronika-zimmer/</link>
    </image>
    
    <item>
      <title>MSc Thesis: Distributionally robust neural networks in medical imaging</title>
      <link>https://aim-lab.io/theses/robust/</link>
      <pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/robust/</guid>
      <description>&lt;p&gt;Deep learning has revolutionized the field of medical imaging. However, the performance of a model drops when the distribution of the test data is different from the distribution of the training data. This is especially critical in a medical setting, where the amount of labelled training data is often limited, and in particular for the application in clinical routine, where the distribution of the test data cannot be controlled. Typical problems are here domain shift, bias in the training data and out-of-distribution samples (e.g., pathologies, image artefacts).
In this project, we will explore distributionally robust optimization (DRO) [1,2] for deep learning in medical imaging. Instead of minimizing the average loss of a training set, DRO minimizes the worst-case risk and with this optimizes the performances on ”hard” examples. The student will adapt and develop novel methods using DRO for medical imaging applications (classification, segmentation and/or registration.&lt;/p&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Prior experience and good understanding in machine learning and statistics.&lt;/li&gt;
&lt;li&gt;Very good programming skills in python (and pytorch).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;J. Duchi and H. Namkoong: Learning models with uniform performance via distributionally robust optimization. arXiv preprint arXiv:1810.08750 (2018)&lt;/li&gt;
&lt;li&gt;S. Sagawa et al.: Distributionally Robust Neural Networks, In Proc. ICLR (2020)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>MSc Thesis: Out-of-distribution detection using contrastive training for medical imaging</title>
      <link>https://aim-lab.io/theses/oodd/</link>
      <pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/oodd/</guid>
      <description>&lt;p&gt;Deep learning has revolutionized the field of medical imaging. However, the performance of a model drops when the distribution of the test data is different from the distribution of the training data. This is especially critical in a medical setting, where the amount of labelled training data is often limited, and in particular for the application in clinical routine, where the distribution of the test data cannot be controlled. The automatic detection of such Out-of-Distribution (OoD) samples at inference is important for the design of reliable models, but also to identify poor quality images and pathologies not seen during training.&lt;/p&gt;
&lt;p&gt;Recently, contrastive learning has shown to provide state-of-the-art results for OoD in image classification benchmarks [1]. Contrastive learning is an approach to formulate the task of finding similar and dissimilar samples during training. One advantage of the proposed method is that no OoD data is required during training.
The aim of this project is to explore OoD detection in deep learning in general, and in particular the use of contrastive training. The student will develop and implement (novel) methods for image classification, segmentation, and/or registration in a medical application.&lt;/p&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Prior experience and good understanding in machine learning and statistics.&lt;/li&gt;
&lt;li&gt;Very good programming skills in python (and pytorch).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;J. Winkens et al.: Contrastive Training for Improved Out-of-Distribution Detection. arXiv preprint arXiv:2007.05566 (2020)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
