<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julian McGinnis | AI in Medicine</title>
    <link>https://aim-lab.io/author/julian-mcginnis/</link>
      <atom:link href="https://aim-lab.io/author/julian-mcginnis/index.xml" rel="self" type="application/rss+xml" />
    <description>Julian McGinnis</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Technical University of Munich 2023</copyright><lastBuildDate>Fri, 16 Jun 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://aim-lab.io/images/icon_hu90763c276d9f69c3ad22e431a6bb6670_11797_512x512_fill_lanczos_center_3.png</url>
      <title>Julian McGinnis</title>
      <link>https://aim-lab.io/author/julian-mcginnis/</link>
    </image>
    
    <item>
      <title>MSc Thesis: Disentangled Implicit Neural Representations</title>
      <link>https://aim-lab.io/theses/mcginnis/mcginnis_disentangled/</link>
      <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/mcginnis/mcginnis_disentangled/</guid>
      <description>&lt;p&gt;Implicit Neural Representations (INR) have emerged as powerful and compact representations for audio, shapes and images, and have recently been translated into medical applications for super-resolution, reconstruction or shape completion [1]. While INRs have been traditionally trained on a single scene or image in the past, novel approaches such as [2] allow to capture a dataset of similar entities with the help of latent representations. Based upon this, INRs are emerging in generative settings and other areas of deep learning, making them a novel and pioneering field [3].&lt;/p&gt;
&lt;p&gt;Built-up on these concepts, our current research focuses on efficient and interpretable implicit representations that may be generalized to diverse datasets and settings. Our aim is to find alternative methods and algorithms to [2], allowing to disentangle image features from spatial occurrences.
&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;your-qualifications&#34;&gt;Your qualifications:&lt;/h2&gt;
&lt;p&gt;We are looking for a highly motivated Master’s student in Computer Science, Physics, Engineering or Mathematics. Your goal is to extend the existing Pytorch codebase by incorporating novel algorithms and methods, and work on signal processing methodology. Importantly, we aim to publish the results of this work, with you, in a follow up study at a high-impact machine learning conference or in an academic journal.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Strong motivation and interest in machine learning.&lt;/li&gt;
&lt;li&gt;Advanced programming skills in C++, Python or C.&lt;/li&gt;
&lt;li&gt;Strong interest in teamwork and interdisciplinary research.
&lt;br/&gt;&lt;br/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An exciting research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to state-of-the-art computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of highly qualified experts in machine learning, computer vision and deep learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-to-apply&#34;&gt;How to apply:&lt;/h2&gt;
&lt;p&gt;Please send us a short e-mail with your CV and grade report to &lt;a href=&#34;mailto:julian.mcginnis@tum.de&#34;&gt;julian.mcginnis@tum.de&lt;/a&gt; and &lt;a href=&#34;mailto:suprosanna.shit@tum.de&#34;&gt;suprosanna.shit@tum.de&lt;/a&gt;.
&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] McGinnis J, Shit S, Li HB, Sideri-Lampretsa V, Graf R, Dannecker M, Pan J, Ansó NS, Mühlau M, Kirschke JS, Rueckert D. Multi-contrast MRI Super-resolution via Implicit Neural Representations. &lt;a href=&#34;https://arxiv.org/pdf/2303.15065.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2303.15065.pdf&lt;/a&gt; &lt;/br&gt;
[2] Mehta, Ishit, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan Chandraker. &amp;ldquo;Modulated periodic activations for generalizable local functional representations.&amp;rdquo; In &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;, pp. 14214-14223. 2021. &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Mehta_Modulated_Periodic_Activations_for_Generalizable_Local_Functional_Representations_ICCV_2021_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openaccess.thecvf.com/content/ICCV2021/papers/Mehta_Modulated_Periodic_Activations_for_Generalizable_Local_Functional_Representations_ICCV_2021_paper.pdf&lt;/a&gt; &lt;/br&gt;
[3] Dupont, Emilien, Hyunjik Kim, S. M. Eslami, Danilo Rezende, and Dan Rosenbaum. &amp;ldquo;From data to functa: Your data point is a function and you can treat it like one.&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:2201.12204&lt;/em&gt; (2022). &lt;a href=&#34;https://proceedings.mlr.press/v162/dupont22a/dupont22a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://proceedings.mlr.press/v162/dupont22a/dupont22a.pdf&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MSc Thesis: Implicit Neural Representations for Medical Applications</title>
      <link>https://aim-lab.io/theses/mcginnis/mcginnis_medical_inrs/</link>
      <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/mcginnis/mcginnis_medical_inrs/</guid>
      <description>&lt;p&gt;Implicit Neural Representations (INR) have emerged as powerful and compact representations for audio, shapes and images, and have recently been translated into medical applications [1].
Our recent work has shown that INRs can be used in multi-contrast MRI super-resolution, reconstructing high-resolution images from low-resolution scans, while preserving anatomical details and pathological findings, such as lesions and tumors. Notably, these methods require only training data of a single patient, allowing to use these models in the context of patient-specific super-resolution, making them attractive for clinical settings.&lt;/p&gt;
&lt;p&gt;Built upon our recent findings, we are interested in incorporating novel concepts such as modulated periodic activation functions [2] and meta-learning [3], to scale INRs to cohorts of patients by incorporating priors learned over a dataset. Once incorporated, we aim to solve various medical downstream tasks in brain and spinal cord imaging, enabling our clinical partners to work on exciting scientific questions.
&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;your-qualifications&#34;&gt;Your qualifications:&lt;/h2&gt;
&lt;p&gt;We are looking for a highly motivated Master’s student in Computer Science, Physics, Engineering or Mathematics. Your goal is to extend the existing Pytorch codebase by incorporating novel algorithms and methods. Importantly, we aim to publish the results of this work, with you, in a follow up study at a high-impact machine learning conference or in an academic journal.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Strong motivation and interest in machine learning.&lt;/li&gt;
&lt;li&gt;Advanced programming skills in C++, Python or C.&lt;/li&gt;
&lt;li&gt;Strong interest in teamwork and interdisciplinary research.
&lt;br/&gt;&lt;br/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An exciting research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to state-of-the-art computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of highly qualified experts in machine learning, computer vision and deep learning.
&lt;br/&gt;&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-apply&#34;&gt;How to apply:&lt;/h2&gt;
&lt;p&gt;Please send us a short e-mail with your CV and grade report to &lt;a href=&#34;mailto:julian.mcginnis@tum.de&#34;&gt;julian.mcginnis@tum.de&lt;/a&gt; and &lt;a href=&#34;mailto:suprosanna.shit@tum.de&#34;&gt;suprosanna.shit@tum.de&lt;/a&gt;.
&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] McGinnis J, Shit S, Li HB, Sideri-Lampretsa V, Graf R, Dannecker M, Pan J, Ansó NS, Mühlau M, Kirschke JS, Rueckert D. Multi-contrast MRI Super-resolution via Implicit Neural Representations. &lt;a href=&#34;https://arxiv.org/pdf/2303.15065.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2303.15065.pdf&lt;/a&gt; &lt;/br&gt;
[2] Mehta, Ishit, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan Chandraker. &amp;ldquo;Modulated periodic activations for generalizable local functional representations.&amp;rdquo; In &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;, pp. 14214-14223. 2021. &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Mehta_Modulated_Periodic_Activations_for_Generalizable_Local_Functional_Representations_ICCV_2021_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openaccess.thecvf.com/content/ICCV2021/papers/Mehta_Modulated_Periodic_Activations_for_Generalizable_Local_Functional_Representations_ICCV_2021_paper.pdf&lt;/a&gt; &lt;/br&gt;
[3]  Sitzmann, Vincent, Eric Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. &amp;ldquo;Metasdf: Meta-learning signed distance functions.&amp;rdquo; Advances in Neural Information Processing Systems 33 (2020): 10136-10147.21. &lt;a href=&#34;https://papers.nips.cc/paper/2020/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://papers.nips.cc/paper/2020/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf&lt;/a&gt; &lt;/br&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
