<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paul Hager | AI in Medicine</title>
    <link>https://aim-lab.io/author/paul-hager/</link>
      <atom:link href="https://aim-lab.io/author/paul-hager/index.xml" rel="self" type="application/rss+xml" />
    <description>Paul Hager</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© Technical University of Munich 2023</copyright><lastBuildDate>Thu, 01 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://aim-lab.io/author/paul-hager/avatar_hu3e94c61b1072a7596c8f9779db81f911_78763_270x270_fill_q90_lanczos_center.jpg</url>
      <title>Paul Hager</title>
      <link>https://aim-lab.io/author/paul-hager/</link>
    </image>
    
    <item>
      <title>MSc Thesis: Cardiac MRI Segmentation using Morphometric Informed Multimodal Self-Supervised Models</title>
      <link>https://aim-lab.io/theses/morphometric_segmentation/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/morphometric_segmentation/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;Medical datasets and especially biobanks, often contain extensive tabular data with rich clinical information in addition to images. In practice, clinicians typically have less data, both in terms of diversity and scale, but still, wish to deploy deep learning solutions. Combined with increasing medical dataset sizes and expensive annotation costs, the necessity for unsupervised methods that can be pretrain multimodally and predict unimodally has risen.&lt;/p&gt;
&lt;p&gt;We have developed a self-supervised framework that incorporates both tabular and imaging data during pretraining but requires only images during testing. We found that the training dynamics are strongly influenced by morphometric features, i.e. features that describe shape and size, and that these features greatly improve the performance on relevant downstream tasks. We expect segmentation to be a task that benefits from such morphometric features.&lt;/p&gt;
&lt;p&gt;Your goal will be to use the aforementioned multimodal framework to improve the performance of cardiac MR segmentation using UK Biobank cardiac MR data. You will explore the influence of the morphometric features on downstream performance and how different encoder architectures change the pretraining and finetuning dynamics. You will investigate the influence of different imaging planes (Short Axis, Long Axis 2 channel, Long Axis 4 channel) on the pretraining and downstream segmentation performance.&lt;/p&gt;
&lt;h2 id=&#34;to-accomplish-this-work-successfully-we-expect-you-to-have&#34;&gt;To accomplish this work successfully, we expect you to have:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Strong coding skills and familiarity with Pytorch and Numpy&lt;/li&gt;
&lt;li&gt;Basic knowledge of segmentation&lt;/li&gt;
&lt;li&gt;Basic knowledge of contrastive self-supervised learning, especially SimCLR&lt;/li&gt;
&lt;li&gt;A strong spirit of independent work and desire to solve interesting research questions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An exciting research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to state-of-the-art computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of experts in image processing, deep learning, biomedical engineering and medicine.&lt;/li&gt;
&lt;li&gt;Support in bringing your finished project to publication&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-apply&#34;&gt;How to apply:&lt;/h2&gt;
&lt;p&gt;Send an email to &lt;a href=&#34;mailto:paul.hager@tum.de&#34;&gt;paul.hager@tum.de&lt;/a&gt; with your CV and transcript. We promise to get back to you within a couple of days.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MSc Thesis: Tabular Feature Selection and Shared Latent Space Explainability in Self-Supervised Multimodal Deep Learning</title>
      <link>https://aim-lab.io/theses/tabular_feature_selection/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/tabular_feature_selection/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;Medical datasets and especially biobanks, often contain extensive tabular data with rich clinical information in addition to images. In practice, clinicians typically have less data, both in terms of diversity and scale, but still, wish to deploy deep learning solutions. Combined with increasing medical dataset sizes and expensive annotation costs, the necessity for unsupervised methods that can be pretrain multimodally and predict unimodally has risen.&lt;/p&gt;
&lt;p&gt;We have developed a self-supervised framework that incorporates both tabular and imaging data during pretraining but requires only images during testing. So far, during pretraining we have used all possibly relevant features but have indications that there exists an optimal subset. Furthermore, the shared latent space should hold keys to the increased performance of the multimodal pretraining process.&lt;/p&gt;
&lt;p&gt;The goal of this thesis will be to develop a framework for optimal subset selection of tabular features for pretraining. Your target is to improve the downstream performance of the imaging encoder across all tasks. Theoretically, you will explore concepts such as mutual information and collinearity. Experimentally, you will use our network architecture and the UK Biobank dataset of cardiac MR data to test your hypotheses. As an extension, you will use explainability methods such as Guided GradCAM and integrated gradients to investigate how the learned features change with different subsets of tabular features and why perfomance increases.&lt;/p&gt;
&lt;h2 id=&#34;to-accomplish-this-work-successfully-we-expect-you-to-have&#34;&gt;To accomplish this work successfully, we expect you to have:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Strong coding skills and familiarity with Pytorch and Numpy&lt;/li&gt;
&lt;li&gt;Basic knowledge of contrastive self-supervised learning, especially SimCLR&lt;/li&gt;
&lt;li&gt;Basic knowledge of deep-learning explainability methods&lt;/li&gt;
&lt;li&gt;A strong spirit of independent work and desire to solve interesting research questions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An exciting research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to state-of-the-art computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of experts in image processing, deep learning, biomedical engineering and medicine.&lt;/li&gt;
&lt;li&gt;Support in bringing your finished project to publication&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-apply&#34;&gt;How to apply:&lt;/h2&gt;
&lt;p&gt;Send an email to &lt;a href=&#34;mailto:paul.hager@tum.de&#34;&gt;paul.hager@tum.de&lt;/a&gt; with your CV and transcript. We promise to get back to you within a couple of days.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
