<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI in Medicine</title>
    <link>https://aim-lab.io/</link>
      <atom:link href="https://aim-lab.io/index.xml" rel="self" type="application/rss+xml" />
    <description>AI in Medicine</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Technical University of Munich 2024</copyright><lastBuildDate>Tue, 05 Nov 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://aim-lab.io/images/icon_hu90763c276d9f69c3ad22e431a6bb6670_11797_512x512_fill_lanczos_center_2.png</url>
      <title>AI in Medicine</title>
      <link>https://aim-lab.io/</link>
    </image>
    
    <item>
      <title>MSc Thesis: Making LLMs Localize Objects</title>
      <link>https://aim-lab.io/theses/phillipmuller/master_locllm_index/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/phillipmuller/master_locllm_index/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: We want to integrate specialized localization components into LLM-based VLMs to improve their localization capabilities and overall performance on localized tasks such as object detection, referring expression generation.&lt;/p&gt;
&lt;br/&gt;
&lt;h3 id=&#34;description&#34;&gt;Description&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Vision-Language Models (VLMs)&lt;/strong&gt; integrate computer vision and natural language processing
approaches to handle both images and text in a single model. Typical tasks include image captioning (i.e., predicting textual descriptions for images) and visual question answering (i.e., answering questions related to images).&lt;/p&gt;
&lt;p&gt;State-of-the-art VLMs like Llama 3.2 &lt;strong&gt;[1]&lt;/strong&gt; or LLaVa &lt;strong&gt;[2,3]&lt;/strong&gt; integrate pre-trained image encoders into &lt;strong&gt;LLMs&lt;/strong&gt; using adapter layers that project image features into the LLM token space. Utilizing the concepts learned by LLMs, these models showcase strong results on vision-language tasks.&lt;/p&gt;
&lt;p&gt;However, these models are quite limited in their &lt;strong&gt;localization&lt;/strong&gt; capabilities. While some works &lt;strong&gt;[4-7]&lt;/strong&gt; use coordinate tokens to predict bounding boxes, the LLMs are not specialized for localization tasks, thus typically underperforming specialized models on localization tasks such as &lt;strong&gt;object detection and referring expression generation&lt;/strong&gt;. Recently, some works have tried integrating localization into VLMs &lt;strong&gt;[8-11]&lt;/strong&gt;. We want to build on these works and integrate specialized localization components into LLM-based VLMs.&lt;/p&gt;
&lt;p&gt;We will be adapting LLaVa &lt;strong&gt;[2]&lt;/strong&gt;-style VLMs with state-of-the-art LLMs and integrating DETR &lt;strong&gt;[12]&lt;/strong&gt;-style object detectors. Note that this work focuses on natural images but may be evaluated later on medical tasks.&lt;/p&gt;
&lt;br/&gt;
&lt;h3 id=&#34;your-qualifications&#34;&gt;Your qualifications&lt;/h3&gt;
&lt;p&gt;We are looking for a highly motivated Master’s student in Computer Science, Physics, Engineering, Mathematics, or a related field.&lt;/p&gt;
&lt;p&gt;You will establish a new model and training pipeline in PyTorch and evaluate the model on several benchmarks.&lt;/p&gt;
&lt;p&gt;You will work at the Institute for AI in Medicine at TU Munich. Importantly, we aim to publish the results of this work with you in a follow-up study at a high-impact computer vision conference or in an academic journal (e.g., CVPR, ICCV, &amp;hellip;) .&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Strong motivation and interest in machine learning and computer vision.&lt;/li&gt;
&lt;li&gt;Advanced programming skills in Python and a common DL framework (PyTorch, Tensorflow, JAX), preferably PyTorch.&lt;/li&gt;
&lt;li&gt;Independent working style with a strong interest in teamwork and methodic research.&lt;/li&gt;
&lt;li&gt;(Optional) Prior experience with LLMs or object detection is a plus but not required.&lt;/li&gt;
&lt;/ol&gt;
&lt;br/&gt;
&lt;h3 id=&#34;what-we-offer&#34;&gt;What we offer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An exciting state-of-the-art research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to the necessary computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of highly qualified experts in machine learning, computer vision, and deep learning.&lt;/li&gt;
&lt;li&gt;The chance to publish the work in high-impact venues or journals.&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;
&lt;h3 id=&#34;how-to-apply&#34;&gt;&lt;strong&gt;How to apply&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Please send your CV and transcript to Philip Müller (&lt;a href=&#34;mailto:philip.j.mueller@tum.de&#34;&gt;philip.j.mueller@tum.de&lt;/a&gt;) using the subject “&lt;strong&gt;MSc Thesis: Making LLMs Localize Objects”&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Please also include brief summaries of your prior deep learning projects (including your contributions to the project and the used framework) and, if available, provide links to the codebases (e.g., your GitHub profile).&lt;/p&gt;
&lt;br/&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Meta. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. 2024 (&lt;a href=&#34;https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/&#34;&gt;https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[2] Liu et al. Visual Instruction Tuning. NeurIPS 2023 (&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;https://arxiv.org/abs/2304.08485&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[3] Liu et al. Improved Baselines with Visual Instruction Tuning. CVPR 2024 (&lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;https://arxiv.org/abs/2310.03744&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[4] Chen et al. Pix2seq: A Language Modeling Framework for Object Detection. ICLR 2022 (&lt;a href=&#34;https://arxiv.org/abs/2109.10852&#34;&gt;https://arxiv.org/abs/2109.10852&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[5] Yang et al. UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling. ECCV 2022 (&lt;a href=&#34;https://arxiv.org/abs/2111.12085&#34;&gt;https://arxiv.org/abs/2111.12085&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[6] Peng et al. Kosmos-2: Grounding Multimodal Large Language Models to the World. ICLR 2023 (&lt;a href=&#34;https://arxiv.org/abs/2306.14824&#34;&gt;https://arxiv.org/abs/2306.14824&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[7] Bannur et al. MAIRA-2: Grounded Radiology Report Generation. 2024 (&lt;a href=&#34;https://arxiv.org/abs/2406.04449&#34;&gt;https://arxiv.org/abs/2406.04449&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[8] Lai et al. LISA: Reasoning Segmentation via Large Language Model. CVPR 2024 (&lt;a href=&#34;https://arxiv.org/abs/2308.00692&#34;&gt;https://arxiv.org/abs/2308.00692&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[9] Rasheed et al. GLaMM: Pixel Grounding Large Multimodal Model. CVPR 2024 (&lt;a href=&#34;https://arxiv.org/abs/2311.03356&#34;&gt;https://arxiv.org/abs/2311.03356&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[10] Zhang et al. PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model. 2024 (&lt;a href=&#34;https://arxiv.org/abs/2403.14598&#34;&gt;https://arxiv.org/abs/2403.14598&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[11] Müller et al. ChEX: Interactive Localization and Region Description in Chest X-rays. ECCV 2024 (&lt;a href=&#34;https://arxiv.org/abs/2404.15770&#34;&gt;https://arxiv.org/abs/2404.15770&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;[12] Carion et al. End-to-End Object Detection with Transformers. ECCV 2020 (&lt;a href=&#34;https://arxiv.org/abs/2005.12872&#34;&gt;https://arxiv.org/abs/2005.12872&lt;/a&gt;)&lt;/p&gt;
&lt;br/&gt;
</description>
    </item>
    
    <item>
      <title>AIM Lab @ MICCAI 2024</title>
      <link>https://aim-lab.io/post/miccai_2024/</link>
      <pubDate>Thu, 10 Oct 2024 13:45:38 +0200</pubDate>
      <guid>https://aim-lab.io/post/miccai_2024/</guid>
      <description>&lt;p&gt;Members of the AIM lab had a a fantastic time discussing science and enjoying Marrakech at MICCAI 2024. We presented over 20 scientific papers at the main MICCAI conference and the workshops in the area of image reconstruction, image analysis and multi-modal machine learning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MICCAI Enduring Impact Award</title>
      <link>https://aim-lab.io/post/enduring_impact_award/</link>
      <pubDate>Thu, 10 Oct 2024 12:45:38 +0200</pubDate>
      <guid>https://aim-lab.io/post/enduring_impact_award/</guid>
      <description>&lt;p&gt;The Medical Image Computing and Computer Assisted Intervention Society (The MICCAI Society) presents Professor Daniel Rückert with the Enduring Impact Award 2024. He received the award at the MICCAI conference, which took place from October 6 to 10, 2024, in Marrakech, Morocco.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cit.tum.de/en/cit/news/article/miccai-enduring-impact-award-daniel-rueckert/&#34;&gt;https://www.cit.tum.de/en/cit/news/article/miccai-enduring-impact-award-daniel-rueckert/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MSc Thesis: Large Language Models in Medicine</title>
      <link>https://aim-lab.io/theses/martonszep/index_llm-med/</link>
      <pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/martonszep/index_llm-med/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description:&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) have shown exceptional capabilities in understanding and generating human-like text. In the medical field, these models hold the potential to revolutionize patient care, medical research, and healthcare administration. By leveraging LLMs, we can enhance diagnostic accuracy, streamline administrative tasks, and provide personalized patient care. However, the integration of these models into medical practice poses unique challenges and opportunities that warrant thorough investigation.
Objective&lt;/p&gt;
&lt;p&gt;The primary objective of this thesis is to develop and evaluate AI systems powered by Large Language Models that aid medical professionals in their work. Specific research topics and applications will be agreed upon with the student, allowing for a tailored approach that aligns with the student&amp;rsquo;s interests and expertise. Potential areas of focus include (but are not limited to):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLMs and Knowledge Graphs: Integrating LLMs with medical knowledge graphs to improve information retrieval and decision support.&lt;/li&gt;
&lt;li&gt;Retrieval-Augmented Generation: Enhancing the generation capabilities of LLMs by incorporating external data sources.&lt;/li&gt;
&lt;li&gt;Explainability: Developing methods to interpret and explain the outputs of LLMs to ensure transparency and trustworthiness in medical applications.&lt;/li&gt;
&lt;li&gt;Uncertainty Quantification: Quantifying and communicating the uncertainty in LLM predictions to assist medical professionals in making informed decisions.
&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Literature review: A comprehensive literature review will be conducted to understand the current state of LLMs in medicine from the chosen perspective.&lt;/li&gt;
&lt;li&gt;Data collection and preprocessing: Utilizing real clinical data in German, provided by the institution. The data will be preprocessed and anonymized to ensure compliance with privacy regulations. If needed, this will be extended with public data.&lt;/li&gt;
&lt;li&gt;Explore, implement and compare different approaches: Train and fine-tune LLMs.&lt;/li&gt;
&lt;li&gt;Evaluation: Assessing the performance of the developed systems through quantitative metrics and qualitative feedback from medical experts.&lt;/li&gt;
&lt;li&gt;Discussion and presentation of results
&lt;br/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Language skills: Good English and at least intermediate German language skills are necessary for working with clinical data.&lt;/li&gt;
&lt;li&gt;Technical skills: Advanced knowledge of deep learning, proficiency in Python and experience with PyTorch is essential. Familiarity with the HuggingFace ecosystem is a plus.&lt;/li&gt;
&lt;li&gt;Medical data experience: Previous experience with medical data is a beneficial.
&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Access to real clinical data: The thesis will provide access to unique clinical data with high potential for publication and impactful research.&lt;/li&gt;
&lt;li&gt;Interdisciplinary environment: Students will work in a highly educated and interdisciplinary team, fostering collaboration between computer science and medical experts.&lt;/li&gt;
&lt;li&gt;Advanced computing resources: The institution offers top-level hardware for scientific computing, ensuring efficient and effective model training and experimentation.&lt;/li&gt;
&lt;li&gt;Expert feedback: Continuous feedback from both medical professionals and computer science experts will guide the research process, ensuring the development of relevant and high-quality solutions.
&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-apply&#34;&gt;How to apply:&lt;/h2&gt;
&lt;p&gt;Preferred start date: September 2024, with flexibility&lt;/p&gt;
&lt;p&gt;Send an email to &lt;a href=&#34;mailto:marton.szep@tum.de&#34;&gt;marton.szep@tum.de&lt;/a&gt;, with your CV, transcript of records, and a small introduction about you and your motivation.
&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;p&gt;Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., &amp;amp; Wu, X. (2024). Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering.&lt;/p&gt;
&lt;p&gt;Zhang, B., &amp;amp; Soh, H. (2024). Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction. arXiv preprint arXiv:2404.03868.&lt;/p&gt;
&lt;p&gt;Cao, L., Sun, J., &amp;amp; Cross, A. (2024). AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models. arXiv preprint arXiv:2403.00953.&lt;/p&gt;
&lt;p&gt;Kommineni, V. K., König-Ries, B., &amp;amp; Samuel, S. (2024). From human experts to machines: An LLM supported approach to ontology and knowledge graph construction. arXiv preprint arXiv:2403.08345.
&lt;br/&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MSc Thesis: Leveraging Differential Privacy to Learn General and Robust Deep Learning Models</title>
      <link>https://aim-lab.io/theses/florianhoelzl/thesis-improving-dp/</link>
      <pubDate>Wed, 17 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/florianhoelzl/thesis-improving-dp/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;Deep learning aims at learning general representations of data allowing for downstream tasks such as classification, regression or generation of new data. In practice, however, there are no formal guarantees to what a model learns, resulting in unwanted memorisation of input data and leaking of private information. Differential privacy (DP) is the gold standard of privacy-preserving deep learning, offering a formal guarantee for protecting sensitive information and thus as a consequence learning general representations. Privacy in deep learning, however, still comes at a high privacy-utility trade-off that restricts the practical usability of models trained under DP [1]. We, as researchers, try to solve this issue by looking into learning theory and by developing novel approaches that allow DP networks to challenge non-private approaches.&lt;/p&gt;
&lt;p&gt;The privacy-utility trade-off of DP at tight privacy bounds is a main reason that keeps DP from being used in practical application. The work aims at improving the substantially worse prediction performance when training with DP-SGD. There has been an increasing focus in recent research on mitigating the bias introduced by gradient clipping and the destructive impact of noise during convergence [2, 3]. The first work package focuses on reimplementing a baseline approach from literature in order to find a suitable quantitative set of metrics [4, 5] to analyse the impact of adaptive approaches on optimisation in comparison to standard SGD and DP-SGD  [6, 7, 8, 9, 10, 11]. In the second work package, we will further include more approaches from current literature and use the previously identified metrics to evaluate and compare them based on their impact on learning general and private representations. In a third work package, this will ideally allow for identifying key factors for improving optimisation under DP, correspondingly improving current training regimes and reaching performance similar to non-private approaches.&lt;/p&gt;
&lt;p&gt;If successful, this work will give a new perspective on the deep learning optimisation and allow for establishing DP as a go to approach in practice to learn general and accurate deep learning models.&lt;/p&gt;
&lt;h2 id=&#34;your-qualifications&#34;&gt;Your qualifications&lt;/h2&gt;
&lt;p&gt;We are looking for a highly motivated Master’s student in Computer Science, Physics, Engineering or Mathematics. You will establish a comprehensive pipeline in PyTorch or JAX to evaluate the model sparsity and changing privacy-parameters on different benchmark datasets for models trained under DP. You will be working at the institute for AI in Medicine, at the Privacy-Preserving and Trustworthy Machine Learning Group. Importantly, we aim to publish the results of this work, with you, in a follow up study at a high-impact deep learning conference or in an academic journal.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong motivation and interest in deep learning and learning theory.&lt;/li&gt;
&lt;li&gt;Advanced programming skills in Python and a common DL framework (PyTorch, Tensorflow, JAX).&lt;/li&gt;
&lt;li&gt;Independent working style with strong interest in teamwork and methodic research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An exciting state-of-the-art research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to the necessary computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of highly qualified experts in image processing, computer vision and deep learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Monir et al. (2024). A Review of Adaptive Techniques and Data Management Issues in DP-SGD. IEEE 2024.&lt;/li&gt;
&lt;li&gt;Xiao et al. (2023). A Theory to Instruct Differentially-Private Learning via Clipping Bias Reduction. IEEE 2023.&lt;/li&gt;
&lt;li&gt;Watson et al. (2023). Inference and Interference: The Role of Clipping, Pruning and Loss Landscapes in Differentially Private Stochastic Gradient Descent. ArXiv, abs/2311.06839.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Total_variation_denoising&#34;&gt;https://en.wikipedia.org/wiki/Total_variation_denoising&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Watson et al. (2023). Inference and Interference: The Role of Clipping, Pruning and Loss Landscapes in Differentially Private Stochastic Gradient Descent. ArXiv, abs/2311.06839.&lt;/li&gt;
&lt;li&gt;Andrew et al. (2019). Differentially Private Learning with Adaptive Clipping. NeurIPS 2021.&lt;/li&gt;
&lt;li&gt;Esipova et al. (2023). Disparate Impact in Differential Privacy from Gradient Misalignment. ICLR 2023.&lt;/li&gt;
&lt;li&gt;Knolle et al. (2023). Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD. TPDP 2023.&lt;/li&gt;
&lt;li&gt;Tang et al. (2023). DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction). AAAI 2024.&lt;/li&gt;
&lt;li&gt;Chilukoti et al. (2023). Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation. ArXiv, abs/2312.02400.&lt;/li&gt;
&lt;li&gt;Xiao et al. (2023). Geometry of Sensitivity: Twice Sampling and Hybrid Clipping in Differential Privacy with Optimal Gaussian Noise and Application to Deep Learning. ACM CCS 2023.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>MSc Thesis: Outperforming CNNs and Transformers on Medical Imaging Tasks with Equivariant Networks</title>
      <link>https://aim-lab.io/theses/florianhoelzl/thesis-outperforming-w-equivariance/</link>
      <pubDate>Wed, 17 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/florianhoelzl/thesis-outperforming-w-equivariance/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;Equivariant convolutions are a novel approach that incorporate additional geometric properties of the input domain during the convolution process (i.e. symmetry properties such as rotations and reflections) [1]. This additional inductive bias allows the model to learn more robust and general features from less data, rendering them highly promising for application in the medical domain. So far, however, nobody has investigated how their beneficial characteristics impact the design and scaling of deep learning model architectures.&lt;/p&gt;
&lt;p&gt;We have developed a framework for initial investigation of equivariant convolutions and now want to evaluate how their performance can be further increased with larger model sizes and under different training regimes. This work plans to build onto previous highly impactful research on model design, such as the establishment of the EfficientNet architecture and the Chinchilla language model, to create a sound basis for the further consolidation of equivariant convolutions in the broader research landscape [2, 3]. The focus will be to thoroughly analyse and expand a framework for equivariant models already developed at our lab. This includes getting familiar with model architectures in general, the properties of equivariant convolutions and the intersection of the two. Our goal is to develop a family of equivariant model architectures that can be easily adapted to the properties of the used dataset and are able to beat state-of-the-art non-equivariant alternatives [4].&lt;/p&gt;
&lt;h2 id=&#34;your-qualifications&#34;&gt;Your qualifications&lt;/h2&gt;
&lt;p&gt;We are looking for a highly motivated Master’s student in Computer Science, Physics, Engineering or Mathematics. You will establish a comprehensive pipeline in PyTorch or JAX to evaluate the properties of equivariant models on different benchmark datasets. You will be working at the institute for AI in Medicine, at the Privacy-Preserving and Trustworthy Machine Learning Group. Importantly, we aim to publish the results of this work, with you, in a follow up study at a high-impact deep learning conference or in an academic journal.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong motivation and interest in deep learning and learning theory.&lt;/li&gt;
&lt;li&gt;Advanced programming skills in Python and a common DL framework (PyTorch, Tensorflow, JAX).&lt;/li&gt;
&lt;li&gt;Independent working style with strong interest in teamwork and methodic research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An exciting state-of-the-art research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to the necessary computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of highly qualified experts in image processing, computer vision and deep learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Cohen and Welling (2016). Group Equivariant Convolutions. ICML 2016.&lt;/li&gt;
&lt;li&gt;Tan and Le (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML 2019.&lt;/li&gt;
&lt;li&gt;Hoffmann et al. (2022). Training Compute-Optimal Large Language Models. ArXiv, abs/2203.15556.&lt;/li&gt;
&lt;li&gt;Abadi et al. (2016). Deep Learning with Differential Privacy. ACM SIGSAC 2016.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>MSc Thesis: Privacy-Preserving Synthetic Time Series Data of Electronic Health Records</title>
      <link>https://aim-lab.io/theses/florianhoelzl/thesis-synthetic-time-series-data/</link>
      <pubDate>Wed, 17 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/florianhoelzl/thesis-synthetic-time-series-data/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;Anonymizing data means removing or replacing any identifying information from a dataset, such as names or addresses. The aim of anonymization is to protect the privacy of individuals whose data is being collected and processed. However, anonymizing data is not sufficient to guarantee privacy protection. Research has shown that data samples can easily re-identified through a variety of approaches as simply as combining a dataset with other data sources, e.g. publicly available information [1]. This poses a problem for data sharing, especially in highly sensitive domains such as medicine.&lt;/p&gt;
&lt;p&gt;Synthetic data allows to generate new examples that preserve the statistical properties of the original data [2]. In a medical use case, synthetic data could be used to generate realistic but entirely artificial medical records. These synthetic medical records can be used for a variety of different purposes, such as augmenting existing datasets or sharing data without revealing any patient information [3].&lt;/p&gt;
&lt;p&gt;For the later, however, synthetic data doesn’t per se protect the privacy of individual patients. To achieve privacy preservation, we have to apply additional techniques such as differential privacy [4]. The aim of this project is to evaluate synthetic data generation with differential privacy for medical records of multi-variate time series. You will first evaluate current literature of synthetic data generation for multi-variate time series with varying sequence lengths and irregular time differences [5]. In the second work package, an existing data generation pipeline will be re-evaluated and compared to existing synthetisation approaches on a real-world multiple sclerosis dataset. The data will be used to predict disease activity and help treatment decision making for disease-modifying therapies [6]. To measure the prediction capabilities of the synthetically generated private data, its prediction performance is compared against existing baselines from the original data curated during different time intervals. Furthermore, the addition of synthetic data into the existing training process for increased model robustness will be assessed. The third work package consists of improving the pipeline, especially with regard to conditional sampling, and evaluating it on other publicly available datasets.&lt;/p&gt;
&lt;h2 id=&#34;your-qualifications&#34;&gt;Your qualifications&lt;/h2&gt;
&lt;p&gt;We are looking for a highly motivated Master’s student in Computer Science, Physics, Engineering or Mathematics. You will establish a comprehensive pipeline in PyTorch or JAX to generate and evaluate synthetic data from a real world medical dataset under differential privacy. You will be working at the institute for AI in Medicine, at the Privacy-Preserving and Trustworthy Machine Learning Group. Importantly, we aim to publish the results of this work, with you, in a follow up study at a high-impact deep learning conference or in an academic journal.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Strong motivation and interest in deep learning and learning theory.&lt;/li&gt;
&lt;li&gt;Advanced programming skills in Python and a common DL framework (PyTorch, Tensorflow, JAX).&lt;/li&gt;
&lt;li&gt;Independent working style with strong interest in teamwork and methodic research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An exciting state-of-the-art research project with many possibilities to bring in your own ideas.&lt;/li&gt;
&lt;li&gt;Close supervision and access to the necessary computer hardware.&lt;/li&gt;
&lt;li&gt;The chance to work in a team of highly qualified experts in image processing, computer vision and deep learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Narayanan and Shmatikov (2006). How to Break Anonymity of the Netflix Prize Dataset. ArXiv, cs/0610105&lt;/li&gt;
&lt;li&gt;Jordon et al. (2022). Synthetic Data - what, why and how? ArXiv, abs/2205.03257.&lt;/li&gt;
&lt;li&gt;Chen et al. (2021). Synthetic Data in Machine Learning for Medicine and Healthcare. Nature Biomedical Engineering 2021.&lt;/li&gt;
&lt;li&gt;Abadi et al. (2016). Deep Learning with Differential Privacy. ACM SIGSAC 2016.&lt;/li&gt;
&lt;li&gt;Zhang et al. (2022). Sequential models in the synthetic data vault. ArXiv, abs/2207.14406.&lt;/li&gt;
&lt;li&gt;Braune et al. (2022). PHREND®—A Real-World Data-Driven Tool Supporting Clinical Decisions to Optimize Treatment in Relapsing-Remitting Multiple Sclerosis. Frontiers in Digital Health 03.2022.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Master-Seminar: Multi-modal AI for Medicine (IN2107)</title>
      <link>https://aim-lab.io/theses/huaqiqiu/</link>
      <pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/huaqiqiu/</guid>
      <description>&lt;p&gt;This year&amp;rsquo;s seminar will look at aspects of multi-modal machine learning in medicine and healthcare, focusing on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vision language models (VLMs) for medical and healthcare applications&lt;/li&gt;
&lt;li&gt;Generic multi-modal AI models utilising imaging data, clinical reports, lab test results, electronic health records, and genomics&lt;/li&gt;
&lt;li&gt;Foundation models for multi-modal medicine&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives:&lt;/h2&gt;
&lt;p&gt;At the end of the module students should have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a thorough understanding of current research in multi-modal AI in medicine, in particular about foundation models and large vision-language models and their impact in medicine&lt;/li&gt;
&lt;li&gt;After course completion students should be able to apply learned concepts, critically evaluate research works in the area, and be able to conceptualise strategies to tackle the issues discussed&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Each student will choose one paper from a provided list of papers, read it, and give a 15-minute presentation about the paper during the seminar sessions&lt;/li&gt;
&lt;li&gt;All students are expected and highly encouraged to participate in discussions during the seminar sessions&lt;/li&gt;
&lt;li&gt;Each student will then write a 2-page report after presenting and discussing the paper&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h2&gt;
&lt;p&gt;Students are expected to be familiar with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mathematics basics (graduate level):
&lt;ul&gt;
&lt;li&gt;probability theory&lt;/li&gt;
&lt;li&gt;linear algebra&lt;/li&gt;
&lt;li&gt;calculus&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Machine / deep learning basics, e.g. having completed:
&lt;ul&gt;
&lt;li&gt;Machine Learning (IN2064)&lt;/li&gt;
&lt;li&gt;Introduction to Deep Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt; 
&lt;p&gt;Preference might be given to students with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge in deep learning models in medicine, especially vision and/or language models&lt;/li&gt;
&lt;li&gt;Completion of related courses from our chair, e.g.:
&lt;ul&gt;
&lt;li&gt;AI in Medicine I&lt;/li&gt;
&lt;li&gt;AI in Medicine II&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Work experience in AI / Data Science for Medicine &amp;amp; Healthcare&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;information-session-and-sign-up&#34;&gt;Information session and sign-up&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An online information meeting will take place on &lt;strong&gt;15 July, 16:00&lt;/strong&gt; via Zoom (&lt;a href=&#34;https://tum-conf.zoom-x.de/j/64109399034?pwd=zbcYd1t9e91fy3DqfHyG7NULPyMcsl.1&#34;&gt;https://tum-conf.zoom-x.de/j/64109399034?pwd=zbcYd1t9e91fy3DqfHyG7NULPyMcsl.1&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;You can sign up for the course in the matching system (&lt;a href=&#34;https://matching.in.tum.de/m/mwvrjkg/q/fd56hbnn2x&#34;&gt;https://matching.in.tum.de/m/mwvrjkg/q/fd56hbnn2x&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Please fill in the following form in addition to voting in the matching system. The information you provided will help us to evaluate our votes: &lt;a href=&#34;https://forms.gle/xTbgwcFf1ZeaDeXT7&#34;&gt;https://forms.gle/xTbgwcFf1ZeaDeXT7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;MultimodalSeminar2024W.pdf&#34;&gt;Information Slides - 2024/25 Winter Semester&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MIDL Best Paper Award</title>
      <link>https://aim-lab.io/post/best_paper_award_vasiliki/</link>
      <pubDate>Fri, 05 Jul 2024 13:45:38 +0200</pubDate>
      <guid>https://aim-lab.io/post/best_paper_award_vasiliki/</guid>
      <description>&lt;p&gt;We are very excited that our paper “SINR: Spline-enhanced implicit neural representation for multi-modal registration” won got the best oral award in MIDL 2024! Congratulations to Vasiliki Sideri-Lampretsa and all the co-authors.&lt;/p&gt;
&lt;p&gt;Check out the work here: &lt;a href=&#34;https://openreview.net/forum?id=V5XDYSRcQP!&#34;&gt;https://openreview.net/forum?id=V5XDYSRcQP!&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Practical Course: Applied Deep Learning in Medicine</title>
      <link>https://aim-lab.io/theses/alexziller/practical/</link>
      <pubDate>Mon, 24 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/alexziller/practical/</guid>
      <description>&lt;p&gt;In this course students are given the chance to apply their abilities and knowledge in deep learning to real-world medical data. Students will be assigned a medical dataset and in close consultation with medical doctors create a project plan. Deep Learning methods will be applied to solve tasks to achieve the goal that is agreed upon. Datasets will be explored and analysed in several directions and different approaches will be evaluated and compared.
In short this course offers students to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply Deep Learning in the real world&lt;/li&gt;
&lt;li&gt;Work on medical data and potentially help diagnose and analyse health related problems&lt;/li&gt;
&lt;li&gt;Close supervision by PhD students with specialization in AI&lt;/li&gt;
&lt;li&gt;Collaboration with medical experts&lt;/li&gt;
&lt;li&gt;Work on the intersection between medicine and computer science&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Completed at least one or several machine learning or deep learning courses (e.g. Intro to Deep Learning, Advanced Deep Learning, Machine Learning etc) with good grades. Knowledge about augmentation, optimizer, common model architectures, etc.&lt;/li&gt;
&lt;li&gt;Good coding skills in python&lt;/li&gt;
&lt;li&gt;Coding experience in one or more deep learning frameworks (Tensorflow, PyTorch, etc)&lt;/li&gt;
&lt;li&gt;Enthusiasm for the application in the medical field&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ability to tackle applied deep learning projects in a structured manner with a good overview of possibilities&lt;/li&gt;
&lt;li&gt;Gained insight into the problems of medical data&lt;/li&gt;
&lt;li&gt;Final outcome as a useful insight or tool for medical professionals&lt;/li&gt;
&lt;li&gt;If possible outcome will be published in a peer-reviewed venue&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Students will work in teams of three&lt;/li&gt;
&lt;li&gt;Each group will be assigned one medical dataset&lt;/li&gt;
&lt;li&gt;(Bi)weekly meetings with progress reports&lt;/li&gt;
&lt;li&gt;Final presentation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminary-meeting&#34;&gt;Preliminary meeting&lt;/h2&gt;
&lt;p&gt;A preliminary meeting will take place on 04.07.2024 at 14:00 on zoom with the following details:&lt;br&gt;
&lt;a href=&#34;https://tum-conf.zoom.us/j/69075883519&#34;&gt;https://tum-conf.zoom.us/j/69075883519&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meeting ID: 690 7588 3519&lt;br&gt;
Passcode: 850155&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;PracticalPreMeeting-WiSe2425.pdf&#34;&gt;Slides - WS 2024/25&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;PracticalPreMeeting-SoSe24.pdf&#34;&gt;(Outdated)Slides - SoSe 2024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;PracticalPreMeetingWiSe2324.pdf&#34;&gt;(Outdated)Slides - WS 2023/24&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Master Thesis: Deep Learning for Bone Tumor Detection and Segmentation: 2D vs 3D</title>
      <link>https://aim-lab.io/theses/annacurtovilalta/index_bonetumor/</link>
      <pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://aim-lab.io/theses/annacurtovilalta/index_bonetumor/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;
&lt;p&gt;The detection and segmentation of bone tumors using magnetic resonance imaging (MRI) have crucial implications for clinical diagnosis and treatment planning. With the advent of deep learning techniques, there&amp;rsquo;s a growing interest in leveraging these methods to analyze MRI bone tumor images. However, a fundamental question arises: Is 3D volumetric processing superior to traditional 2D slice-by-slice processing in deep learning tasks for MRI bone tumor analysis? This research addresses this question by evaluating the effectiveness of 2D versus 3D deep learning methodologies.
&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology:&lt;/h2&gt;
&lt;p&gt;The methodology involves implementing several deep-learning models to compare the efficacy of 2D and 3D techniques for MRI bone tumor analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Literature review on the current state-of-the-art techniques in 2D and 3D MRI-based detection and segmentation tasks using deep learning.&lt;/li&gt;
&lt;li&gt;Implement tumor detection and segmentation models in both 2D and 3D.&lt;/li&gt;
&lt;li&gt;Explore and implement a hybrid approach that combines the strengths of both 2D and 3D processing methods to achieve superior results.&lt;/li&gt;
&lt;li&gt;Presenting and discussing results.
&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Advanced knowledge of deep learning with imaging data;&lt;/li&gt;
&lt;li&gt;Beneficial but not necessary: experience in medicine/oncology;&lt;/li&gt;
&lt;li&gt;Preferred starting date: September 2024 (with flexibility);
&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-we-offer&#34;&gt;What we offer:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very rare medical data with high potential for publication.&lt;/li&gt;
&lt;li&gt;Highly educated &amp;amp; interdisciplinary environment.&lt;/li&gt;
&lt;li&gt;Top-level hardware for scientific computing.&lt;/li&gt;
&lt;li&gt;Constant feedback from medical and computer science experts
&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-apply&#34;&gt;How to apply:&lt;/h2&gt;
&lt;p&gt;Send an email to &lt;a href=&#34;mailto:anna.curto-vilalta@tum.de&#34;&gt;anna.curto-vilalta@tum.de&lt;/a&gt;, with your CV and small introduction about you and your motivation.
&lt;br/&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;p&gt;He, Avesta, et al. &amp;ldquo;Comparing 3D, 2.5D, and 2D Approaches to Brain Image Auto-Segmentation.&amp;rdquo; Bioengineering 10 (2023): 181.&lt;/p&gt;
&lt;p&gt;Ushinsky, A., et al. &amp;ldquo;A 3D-2D Hybrid U-Net Convolutional Neural Network Approach to Prostate Organ Segmentation of Multiparametric MRI.&amp;rdquo; American Journal of Roentgenology 216, no. 1 (2021): 111-116.&lt;/p&gt;
&lt;p&gt;Wang, H., et al. &amp;ldquo;Mixed 2D and 3D Convolutional Network with Multi-Scale Context for Lesion Segmentation in Breast DCE-MRI.&amp;rdquo; Biomedical Signal Processing and Control 68 (2021): Article no. 102607.
&lt;br/&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alphabet and Google visit AIM Lab</title>
      <link>https://aim-lab.io/post/google_visit/</link>
      <pubDate>Fri, 16 Feb 2024 13:45:38 +0200</pubDate>
      <guid>https://aim-lab.io/post/google_visit/</guid>
      <description>&lt;p&gt;Yesterday, we welcomed the CEO of Alphabet and Google, Sundar Pichai, to TUM and our Lab for AI in Medicine located at the University Hospital rechts der Isar. We were happy to showcase some of the exciting work on AI, cloud computing and genomics at TUM, followed by a fruitful discussion on how we can work together to accelerate the translation of these developments for the benefit of patients. Thank you for taking the time to visit us!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AIM Lab Mountain Retreat</title>
      <link>https://aim-lab.io/post/aimlab_retreat/</link>
      <pubDate>Sat, 10 Feb 2024 13:45:38 +0200</pubDate>
      <guid>https://aim-lab.io/post/aimlab_retreat/</guid>
      <description>&lt;p&gt;We just returned from our lab retreat at the Kitzsteinhorn in Austria. We had a great time discussing science and enjoying the sun and snow!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The developing Human Connectome Project (dHCP) automated resting-state functional processing framework for newborn infants</title>
      <link>https://aim-lab.io/publication/neuroimage2020a/</link>
      <pubDate>Sat, 21 Nov 2020 21:26:19 +0100</pubDate>
      <guid>https://aim-lab.io/publication/neuroimage2020a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Model-Based and Data-Driven Strategies in Medical Image Computing</title>
      <link>https://aim-lab.io/publication/proc-ieee-2020a/</link>
      <pubDate>Mon, 12 Oct 2020 12:39:15 +0200</pubDate>
      <guid>https://aim-lab.io/publication/proc-ieee-2020a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Secure, privacy-preserving and federated machine learning in medical imaging</title>
      <link>https://aim-lab.io/www.nature.com/articles/s42256-020-0186-1</link>
      <pubDate>Mon, 12 Oct 2020 06:59:15 +0200</pubDate>
      <guid>https://aim-lab.io/www.nature.com/articles/s42256-020-0186-1</guid>
      <description></description>
    </item>
    
    <item>
      <title>Impressum</title>
      <link>https://aim-lab.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://aim-lab.io/privacy/</guid>
      <description>&lt;h3&gt;Anschrift&lt;/h3&gt;
Technische Universität München &lt;br&gt;
Arcisstraße 21 &lt;br&gt;
80333 München &lt;br&gt;
Umsatzsteueridentifikationsnummer: DE811193231
&lt;h3&gt;Zuständige Aufsichtsbehörde&lt;/h3&gt;
Bayerisches Staatsministerium für Wissenschaft und Kunst
&lt;h3&gt;Inhaltlich verantwortlich&lt;/h3&gt;
Prof. Dr. Daniel Rückert&lt;br&gt;
Technische Universität München / Klinikum Rechts der Isar&lt;br&gt;
Lehrstuhl für Artificial Intelligence in Medicine and Healthcare&lt;br&gt;
TranslaTUM&lt;br&gt;
Einsteinstraße 25&lt;br&gt;
81675 München&lt;br&gt;
E-Mail: daniel(dot)rueckert(at)tum(dot)de&lt;br&gt;
&lt;h3&gt;Haftungshinweis&lt;/h3&gt;
&lt;p&gt;Trotz sorgfältiger inhaltlicher Kontrolle übernehmen wir keine Haftung für die Inhalte externer Links. Für den Inhalt der verlinkten Seiten sind ausschließlich deren Betreiber verantwortlich. Namentlich gekennzeichnete Beiträge in den Diskussionsbereichen geben die Meinung des Autors wieder. Für die Inhalte der Beiträge sind ausschließlich die Autoren zuständig.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A population-based phenome-wide association study of cardiac and aortic structure and function</title>
      <link>https://aim-lab.io/publication/nat-med-2020a/</link>
      <pubDate>Thu, 01 Jan 1970 01:33:40 +0100</pubDate>
      <guid>https://aim-lab.io/publication/nat-med-2020a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Genetic and functional insights into the fractal structure of the heart</title>
      <link>https://aim-lab.io/publication/nat-2020a/</link>
      <pubDate>Thu, 01 Jan 1970 01:33:40 +0100</pubDate>
      <guid>https://aim-lab.io/publication/nat-2020a/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
