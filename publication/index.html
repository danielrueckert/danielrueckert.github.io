<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 5.0.0-beta.0 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Johannes C. Paetzold">

  
  
  
    
  
  <meta name="description" content="Research Scientist">

  
  <link rel="alternate" hreflang="en-us" href="https://aim-lab.io/publication/">

  







  




  
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  

  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css">

  




  


  
  

  
  <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="AI in Medicine">
  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu90763c276d9f69c3ad22e431a6bb6670_11797_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu90763c276d9f69c3ad22e431a6bb6670_11797_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://aim-lab.io/publication/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@DanielRueckert">
  <meta property="twitter:creator" content="@DanielRueckert">
  
  <meta property="og:site_name" content="AI in Medicine">
  <meta property="og:url" content="https://aim-lab.io/publication/">
  <meta property="og:title" content="Publications | AI in Medicine">
  <meta property="og:description" content="Research Scientist"><meta property="og:image" content="https://aim-lab.io/images/icon_hu90763c276d9f69c3ad22e431a6bb6670_11797_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://aim-lab.io/images/icon_hu90763c276d9f69c3ad22e431a6bb6670_11797_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    <meta property="og:updated_time" content="2024-08-24T00:00:00&#43;00:00">
  

  




  


  





  <title>Publications | AI in Medicine</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  ">

  
  
  
    <script>window.wcDarkLightEnabled = true;</script>
  
  
    <script>const isSiteThemeDark = false;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">AI in Medicine</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">AI in Medicine</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#page_header"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#news"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#teaching"><span>Teaching</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#vacancies"><span>Vacancies</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2024">
              2024
            </option>
            
            <option value=".year-2023">
              2023
            </option>
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/r.-holland./">R. Holland.</a></span>, <span ><a href="/author/o.-leingang/">O. Leingang</a></span>, <span ><a href="/author/h.-bogunovic/">H. Bogunović</a></span>, <span ><a href="/author/s.-riedl/">S. Riedl</a></span>, <span ><a href="/author/l.-fritsche/">L. Fritsche</a></span>, <span ><a href="/author/t.-prevost/">T. Prevost</a></span>, <span ><a href="/author/h.-p.-n.-scholl/">H. P. N. Scholl</a></span>, <span ><a href="/author/u.-schmidt-erfurth/">U. Schmidt-Erfurth</a></span>, <span ><a href="/author/s.-sivaprasad/">S. Sivaprasad</a></span>, <span ><a href="/author/a.-j.-lotery/">A. J. Lotery</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/m.-j.-menten/">M. J. Menten</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Medical Image Analysis, 97: 103296, 2024
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/mia-2024/">Metadata-enhanced contrastive learning from retinal optical coherence tomography images.</a>
  </h3>

  
  <a href="/publication/mia-2024/" class="summary-link">
    <div class="article-style">
      <p>Deep learning has potential to automate screening, monitoring and grading of disease in medical images. Pretraining with contrastive learning enables models to extract robust and generalisable features from natural image datasets, facilitating label-efficient downstream image analysis. However, the direct application of conventional contrastive methods to medical datasets introduces two domain-specific issues. Firstly, several image transformations which have been shown to be crucial for effective contrastive learning do not translate from the natural image to the medical image domain. Secondly, the assumption made by conventional methods, that any two images are dissimilar, is systematically misleading in medical datasets depicting the same anatomy and disease. This is exacerbated in longitudinal image datasets that repeatedly image the same patient cohort to monitor their disease progression over time. In this paper we tackle these issues by extending conventional contrastive frameworks with a novel metadata-enhanced strategy. Our approach employs widely available patient metadata to approximate the true set of inter-image contrastive relationships. To this end we employ records for patient identity, eye position (i.e. left or right) and time series information. In experiments using two large longitudinal datasets containing 170,427 retinal optical coherence tomography (OCT) images of 7912 patients with age-related macular degeneration (AMD), we evaluate the utility of using metadata to incorporate the temporal dynamics of disease progression into pretraining. Our metadata-enhanced approach outperforms both standard contrastive methods and a retinal image foundation model in five out of six image-level downstream tasks related to AMD. We find benefits in both a low-data and high-data regime across tasks ranging from AMD stage and type classification to prediction of visual acuity. Due to its modularity, our method can be quickly and cost-effectively tested to establish the potential benefits of including available metadata in contrastive pretraining.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1016/j.media.2024.103296" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/a.-ziller/">A. Ziller</a></span>, <span ><a href="/author/t.-t.-mueller/">T. T. Mueller</a></span>, <span ><a href="/author/s.-stieger/">S. Stieger</a></span>, <span ><a href="/author/l.-f.-feiner/">L. F. Feiner</a></span>, <span ><a href="/author/j.-brandt/">J. Brandt</a></span>, <span ><a href="/author/r.-braren/">R. Braren</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/g.-kaissis/">G. Kaissis</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Nature Machine Intelligence, in press, 2024.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/nat-mi-2024/">Reconciling privacy and accuracy in AI for medical imaging.</a>
  </h3>

  
  <a href="/publication/nat-mi-2024/" class="summary-link">
    <div class="article-style">
      <p>Artificial intelligence (AI) models are vulnerable to information leakage of their training data, which can be highly sensitive, for example, in medical imaging. Privacy-enhancing technologies, such as differential privacy (DP), aim to circumvent these susceptibilities. DP is the strongest possible protection for training models while bounding the risks of inferring the inclusion of training samples or reconstructing the original data. DP achieves this by setting a quantifiable privacy budget. Although a lower budget decreases the risk of information leakage, it typically also reduces the performance of such models. This imposes a trade-off between robust performance and stringent privacy. Additionally, the interpretation of a privacy budget remains abstract and challenging to contextualize. Here we contrast the performance of artificial intelligence models at various privacy budgets against both theoretical risk bounds and empirical success of reconstruction attacks. We show that using very large privacy budgets can render reconstruction attacks impossible, while drops in performance are negligible. We thus conclude that not using DP at all is negligent when applying artificial intelligence models to sensitive data. We deem our results to lay a foundation for further debates on striking a balance between privacy risks and model performance.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1038/s42256-024-00858-y" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/p.-hager/">P. Hager</a></span>, <span ><a href="/author/f.-jungmann/">F. Jungmann</a></span>, <span ><a href="/author/k.-bhagat/">K. Bhagat</a></span>, <span ><a href="/author/i.-hubrecht/">I. Hubrecht</a></span>, <span ><a href="/author/m.-knauer/">M. Knauer</a></span>, <span ><a href="/author/j.-vielhauer/">J. Vielhauer</a></span>, <span ><a href="/author/r.-braren/">R. Braren</a></span>, <span ><a href="/author/m.-makowski/">M. Makowski</a></span>, <span ><a href="/author/g.-kaisis/">G. Kaisis</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Nature Medicine, in press, 2024.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/nat-med-2024/">Evaluating and mitigating limitations of large language models in clinical decision making.</a>
  </h3>

  
  <a href="/publication/nat-med-2024/" class="summary-link">
    <div class="article-style">
      <p>Clinical decision making is one of the most impactful parts of a physician&rsquo;s responsibilities and stands to benefit greatly from AI solutions and large language models (LLMs) in particular. However, while LLMs have achieved excellent performance on medical licensing exams, these tests fail to assess many skills that are necessary for deployment in a realistic clinical decision making environment, including gathering information, adhering to established guidelines, and integrating into clinical workflows. To understand how useful LLMs are in real-world settings, we must evaluate them in the wild, i.e. on real-world data under realistic conditions. Here we have created a curated dataset based on the MIMIC-IV database spanning 2400 real patient cases and four common abdominal pathologies as well as a framework to simulate a realistic clinical setting. We show that current state-of-the-art LLMs do not accurately diagnose patients across all pathologies (performing significantly worse than physicians on average), follow neither diagnostic nor treatment guidelines, and cannot interpret laboratory results, thus posing a serious risk to the health of patients. Furthermore, we move beyond diagnostic accuracy and demonstrate that they cannot be easily integrated into existing workflows because they often fail to follow instructions and are sensitive to both the quantity and order of information. Overall, our analysis reveals that LLMs are currently not ready for clinical deployment while providing a dataset and framework to guide future studies.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1038/s41591-024-03097-1" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/i.-lagogiannis/">I. Lagogiannis</a></span>, <span ><a href="/author/f.-meissen/">F. Meissen</a></span>, <span ><a href="/author/g.-kaissis/">G. Kaissis</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE transactions on medical imaging, 43(1): 241-252 2024
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/tmi-2024b/">Unsupervised Pathology Detection: A Deep Dive Into the State of the Art.</a>
  </h3>

  
  <a href="/publication/tmi-2024b/" class="summary-link">
    <div class="article-style">
      <p>Deep unsupervised approaches are gathering increased attention for applications such as pathology detection and segmentation in medical images since they promise to alleviate the need for large labeled datasets and are more generalizable than their supervised counterparts in detecting any kind of rare pathology. As the Unsupervised Anomaly Detection (UAD) literature continuously grows and new paradigms emerge, it is vital to continuously evaluate and benchmark new methods in a common framework, in order to reassess the state-of-the-art (SOTA) and identify promising research directions. To this end, we evaluate a diverse selection of cutting-edge UAD methods on multiple medical datasets, comparing them against the established SOTA in UAD for brain MRI. Our experiments demonstrate that newly developed feature-modeling methods from the industrial and medical literature achieve increased performance compared to previous work and set the new SOTA in a variety of modalities and datasets. Additionally, we show that such methods are capable of benefiting from recently developed self-supervised pre-training algorithms, further increasing their performance. Finally, we perform a series of experiments in order to gain further insights into some unique characteristics of selected models and datasets.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/tmi.2023.3298093" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/c.-i.-bercea/">C. I. Bercea</a></span>, <span ><a href="/author/b.-wiestler/">B. Wiestler</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/j.-a.-schnabel/">J. A. Schnabel</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Nature Communications, in press, 2024.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/nat-com-2024/">Evaluating Normative Representation Learning in Generative AI for Robust Anomaly Detection in Brain Imaging.</a>
  </h3>

  
  <a href="/publication/nat-com-2024/" class="summary-link">
    <div class="article-style">
      <p>In Generative Artificial Intelligence (AI) for medical imaging, normative learning involves training AI models on large datasets of typical images from healthy volunteers, such as MRIs or CT scans. These models acquire the distribution of normal anatomical structures, allowing them to effectively detect and correct anomalies in new, unseen pathological data. This approach allows the detection of unknown pathologies without the need for expert labeling.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.21203/rs.3.rs-3749187/v1" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/r.-raab/">R. Raab</a></span>, <span ><a href="/author/a.-kuderle/">A. Küderle</a></span>, <span ><a href="/author/a.-zakreuskaya/">A. Zakreuskaya</a></span>, <span ><a href="/author/a.-d.-stern/">A. D. Stern</a></span>, <span ><a href="/author/j.-klucken/">J. Klucken</a></span>, <span ><a href="/author/g.-kaissis/">G. Kaissis</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/s.-boll/">S. Boll</a></span>, <span ><a href="/author/r.-eils/">R. Eils</a></span>, <span ><a href="/author/h.-wagener/">H. Wagener</a></span>, <span ><a href="/author/b.-m.-eskofier/">B. M. Eskofier</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Lancet Digital Health, 5: e840–47, 2023
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ldh-2023/">Federated electronic health records for the European Health.</a>
  </h3>

  
  <a href="/publication/ldh-2023/" class="summary-link">
    <div class="article-style">
      <p>The European Commission&rsquo;s draft for the European Health Data Space (EHDS) aims to empower citizens to access their personal health data and share it with physicians and other health-care providers. It further defines procedures for the secondary use of electronic health data for research and development. Although this planned legislation is undoubtedly a step in the right direction, implementation approaches could potentially result in centralised data silos that pose data privacy and security risks for individuals. To address this concern, we propose federated personal health data spaces, a novel architecture for storing, managing, and sharing personal electronic health records that puts citizens at the centre—both conceptually and technologically. The proposed architecture puts citizens in control by storing personal health data on a combination of personal devices rather than in centralised data silos. We describe how this federated architecture fits within the EHDS and can enable the same features as centralised systems while protecting the privacy of citizens. We further argue that increased privacy and control do not contradict the use of electronic health data for research and development. Instead, data sovereignty and transparency encourage active participation in studies and data sharing. This combination of privacy-by-design and transparent, privacy-preserving data sharing can enable health-care leaders to break the privacy-exploitation barrier, which currently limits the secondary use of health data in many cases.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1016/s2589-7500%2823%2900156-5" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/e.-al-jibury/">E. Al-jibury</a></span>, <span ><a href="/author/j.-w.-d.-king/">J. W. D. King</a></span>, <span ><a href="/author/y.-guo/">Y. Guo</a></span>, <span ><a href="/author/b.-lenhard/">B. Lenhard</a></span>, <span ><a href="/author/a.-g.-fisher/">A. G. Fisher</a></span>, <span ><a href="/author/m.-merkenschlager/">M. Merkenschlager</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Nature Communications 14(5007), 2023
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/nat-com-2023/"> A deep learning method for replicate-based analysis of chromosome conformation contacts using Siamese neural networks.</a>
  </h3>

  
  <a href="/publication/nat-com-2023/" class="summary-link">
    <div class="article-style">
      <p>The organisation of the genome in nuclear space is an important frontier of biology. Chromosome conformation capture methods such as Hi-C and Micro-C produce genome-wide chromatin contact maps that provide rich data containing quantitative and qualitative information about genome architecture. Most conventional approaches to genome-wide chromosome conformation capture data are limited to the analysis of pre-defined features, and may therefore miss important biological information. One constraint is that biologically important features can be masked by high levels of technical noise in the data. Here we introduce a replicate-based method for deep learning from chromatin conformation contact maps. Using a Siamese network configuration our approach learns to distinguish technical noise from biological variation and outperforms image similarity metrics across a range of biological systems. The features extracted from Hi-C maps after perturbation of cohesin and CTCF reflect the distinct biological functions of cohesin and CTCF in the formation of domains and boundaries, respectively. The learnt distance metrics are biologically meaningful, as they mirror the density of cohesin and CTCF binding. These properties make our method a powerful tool for the exploration of chromosome conformation capture data, such as Hi-C capture Hi-C, and Micro-C.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1038/s41467-023-40547-9" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/t.-tanida/">T. Tanida</a></span>, <span ><a href="/author/p.-muller/">P. Müller</a></span>, <span ><a href="/author/g.-kaissis/">G. Kaissis</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 7433-7442, 2023.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/cvpr-2023b/">Interactive and Explainable Region-guided Radiology Report Generation.</a>
  </h3>

  
  <a href="/publication/cvpr-2023b/" class="summary-link">
    <div class="article-style">
      <p>The automatic generation of radiology reports has the potential to assist radiologists in the time-consuming task of report writing. Existing methods generate the full report from image-level features, failing to explicitly focus on anatomical regions in the image. We propose a simple yet effective region-guided report generation model that detects anatomical regions and then describes individual, salient regions to form the final report. While previous methods generate reports without the possibility of human intervention and with limited explainability, our method opens up novel clinical use cases through additional interactive capabilities and introduces a high degree of transparency and explainability. Comprehensive experiments demonstrate our method&rsquo;s effectiveness in report generation, outperforming previous state-of-the-art models, and highlight its interactive capabilities.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/cvpr52729.2023.00718" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/q.-meng/">Q. Meng</a></span>, <span ><a href="/author/w.-bai/">W. Bai</a></span>, <span ><a href="/author/d.-p.-oregan/">D. P. O’Regan</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE transactions on medical imaging, 43(4): 1489-1500, 2024
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/tmi-2024a/">DeepMesh: Mesh-based Cardiac Motion Tracking using Deep Learning.</a>
  </h3>

  
  <a href="/publication/tmi-2024a/" class="summary-link">
    <div class="article-style">
      <p>3D motion estimation from cine cardiac magnetic resonance (CMR) images is important for the assessment of cardiac function and the diagnosis of cardiovascular diseases. Current state-of-the art methods focus on estimating dense pixel-/voxel-wise motion fields in image space, which ignores the fact that motion estimation is only relevant and useful within the anatomical objects of interest, e.g., the heart. In this work, we model the heart as a 3D mesh consisting of epi- and endocardial surfaces. We propose a novel learning framework, DeepMesh, which propagates a template heart mesh to a subject space and estimates the 3D motion of the heart mesh from CMR images for individual subjects. In DeepMesh, the heart mesh of the end-diastolic frame of an individual subject is first reconstructed from the template mesh. Mesh-based 3D motion fields with respect to the end-diastolic frame are then estimated from 2D short- and long-axis CMR images. By developing a differentiable mesh-to-image rasterizer, DeepMesh is able to leverage 2D shape information from multiple anatomical views for 3D mesh reconstruction and mesh motion estimation. The proposed method estimates vertex-wise displacement and thus maintains vertex correspondences between time frames, which is important for the quantitative assessment of cardiac function across different subjects and populations. We evaluate DeepMesh on CMR images acquired from the UK Biobank. We focus on 3D motion estimation of the left ventricle in this work. Experimental results show that the proposed method quantitatively and qualitatively outperforms other image-based and mesh-based cardiac motion tracking methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/TMI.2023.3340118" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/a.-marcus/">A. Marcus</a></span>, <span ><a href="/author/p.-bentley/">P. Bentley</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE Transactions on Medical Imaging, 42(12):3464-3473, 2023
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/tmi-2023/">Concurrent ischemic lesion age estimation and segmentation of CT brain using a Transformer-based network.</a>
  </h3>

  
  <a href="/publication/tmi-2023/" class="summary-link">
    <div class="article-style">
      <p>The cornerstone of stroke care is expedient management that varies depending on the time since stroke onset. Consequently, clinical decision making is centered on accurate knowledge of timing and often requires a radiologist to interpret Computed Tomography (CT) of the brain to confirm the occurrence and age of an event. These tasks are particularly challenging due to the subtle expression of acute ischemic lesions and the dynamic nature of their appearance. Automation efforts have not yet applied deep learning to estimate lesion age and treated these two tasks independently, so, have overlooked their inherent complementary relationship. To leverage this, we propose a novel end-to-end multi-task transformer-based network optimized for concurrent segmentation and age estimation of cerebral ischemic lesions. By utilizing gated positional self-attention and CT-specific data augmentation, the proposed method can capture long-range spatial dependencies while maintaining its ability to be trained from scratch under low-data regimes commonly found in medical imaging. Furthermore, to better combine multiple predictions, we incorporate uncertainty by utilizing quantile loss to facilitate estimating a probability density function of lesion age. The effectiveness of our model is then extensively evaluated on a clinical dataset consisting of 776 CT images from two medical centers. Experimental results demonstrate that our method obtains promising performance, with an area under the curve (AUC) of 0.933 for classifying lesion ages less than 4.5 hours compared to 0.858 using a conventional approach, and outperforms task-specific state-of-the-art algorithms.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/TMI.2023.3287361" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/w.-huang/">W. Huang</a></span>, <span ><a href="/author/h.-b.-li/">H. B. Li</a></span>, <span ><a href="/author/j.-pan/">J. Pan</a></span>, <span ><a href="/author/g.-cruz/">G. Cruz</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/k.-hammernik/">K. Hammernik</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Information Processing in Medical Imaging (IPMI), 548-560, 2023.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ipmi-2023/">Neural Implicit k-Space for Binning-Free Non-Cartesian Cardiac MR Imaging.</a>
  </h3>

  
  <a href="/publication/ipmi-2023/" class="summary-link">
    <div class="article-style">
      <p>In this work, we propose a novel image reconstruction framework that directly learns a neural implicit representation in k-space for ECG-triggered non-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods bin acquired data from neighboring time points to reconstruct one phase of the cardiac motion, our framework allows for a continuous, binning-free, and subject-specific k-space representation. We assign a unique coordinate that consists of time, coil index, and frequency domain location to each sampled k-space point. We then learn the subject-specific mapping from these unique coordinates to k-space intensities using a multi-layer perceptron with frequency domain regularization. During inference, we obtain a complete k-space for Cartesian coordinates and an arbitrary temporal resolution. A simple inverse Fourier transform recovers the image, eliminating the need for density compensation and costly non-uniform Fourier transforms for non-Cartesian data. This novel imaging framework was tested on 42 radially sampled datasets from 6 subjects. The proposed method outperforms other techniques qualitatively and quantitatively using data from four and one heartbeat(s) and 30 cardiac phases. Our results for one heartbeat reconstruction of 50 cardiac phases show improved artifact removal and spatio-temporal resolution, leveraging the potential for real-time CMR.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1007/978-3-031-34048-2_42" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/p.-hager/">P. Hager</a></span>, <span ><a href="/author/m.-j.-menten/">M. J. Menten</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 23924-23935, 2023.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/cvpr-2023/">Best of Both Worlds: Multimodal Contrastive Learning With Tabular and Imaging Data.</a>
  </h3>

  
  <a href="/publication/cvpr-2023/" class="summary-link">
    <div class="article-style">
      <p>Medical datasets and especially biobanks, often contain extensive tabular data with rich clinical information in addition to images. In practice, clinicians typically have less data, both in terms of diversity and scale, but still wish to deploy deep learning solutions. Combined with increasing medical dataset sizes and expensive annotation costs, the necessity for unsupervised methods that can pretrain multimodally and predict unimodally has risen. To address these needs, we propose the first self-supervised contrastive learning framework that takes advantage of images and tabular data to train unimodal encoders. Our solution combines SimCLR and SCARF, two leading contrastive learning strategies, and is simple and effective. In our experiments, we demonstrate the strength of our framework by predicting risks of myocardial infarction and coronary artery disease (CAD) using cardiac MR images and 120 clinical features from 40,000 UK Biobank subjects. Furthermore, we show the generalizability of our approach to natural images using the DVM car advertisement dataset. We take advantage of the high interpretability of tabular data and through attribution and ablation experiments find that morphometric tabular features, describing size and shape, have outsized importance during the contrastive learning process and improve the quality of the learned embeddings. Finally, we introduce a novel form of supervised contrastive learning, label as a feature (LaaF), by appending the ground truth label as a tabular feature during multimodal pretraining, outperforming all supervised contrastive baselines.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.48550/arXiv.2303.14080" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/c.-ouyang/">C. Ouyang</a></span>, <span ><a href="/author/c.-chen/">C. Chen</a></span>, <span ><a href="/author/s.-li/">S. Li</a></span>, <span ><a href="/author/z.-li/">Z. Li</a></span>, <span ><a href="/author/c.-qin/">C. Qin</a></span>, <span ><a href="/author/w.-bai/">W. Bai</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE Transactions on Medical Imaging, 42(4):1095-1106, 2023
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/tmi-2024c/">Causality-inspired Single-source Domain Generalization for Medical Image Segmentation.</a>
  </h3>

  
  <a href="/publication/tmi-2024c/" class="summary-link">
    <div class="article-style">
      <p>Deep learning models usually suffer from the domain shift issue, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data are only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. In this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation scenarios: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-site prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/TMI.2022.3224067" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/cchen/">CChen</a></span>, <span ><a href="/author/c.-qin/">C. Qin</a></span>, <span ><a href="/author/c.-ouyang/">C. Ouyang</a></span>, <span ><a href="/author/z.-li/">Z. Li</a></span>, <span ><a href="/author/s.-wang/">S. Wang</a></span>, <span ><a href="/author/h.-qiu/">H. Qiu</a></span>, <span ><a href="/author/l.-chen/">L. Chen</a></span>, <span ><a href="/author/g.-tarroni/">G. Tarroni</a></span>, <span ><a href="/author/w.-bai/">W. Bai</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Medical Image Analysis, 82, 2022.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/mia-2022/">Enhancing MR image segmentation with realistic adversarial data augmentation.</a>
  </h3>

  
  <a href="/publication/mia-2022/" class="summary-link">
    <div class="article-style">
      <p>The success of neural networks on medical image segmentation tasks typically relies on large labeled datasets for model training. However, acquiring and manually labeling a large medical image set is resource-intensive, expensive, and sometimes impractical due to data sharing and privacy issues. To address this challenge, we propose AdvChain, a generic adversarial data augmentation framework, aiming at improving both the diversity and effectiveness of training data for medical image segmentation tasks. AdvChain augments data with dynamic data augmentation, generating randomly chained photo-metric and geometric transformations to resemble realistic yet challenging imaging variations to expand training data. By jointly optimizing the data augmentation model and a segmentation network during training, challenging examples are generated to enhance network generalizability for the downstream task. The proposed adversarial data augmentation does not rely on generative networks and can be used as a plug-in module in general segmentation networks. It is computationally efficient and applicable for both low-shot supervised and semi-supervised learning. We analyze and evaluate the method on two MR image segmentation tasks: cardiac segmentation and prostate segmentation with limited labeled data. Results show that the proposed approach can alleviate the need for labeled data while improving model generalization ability, indicating its practical value in medical imaging applications.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1016/j.media.2022.102597" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/c.-qin/">C. Qin</a></span>, <span ><a href="/author/s.-wang/">S. Wang</a></span>, <span ><a href="/author/c.-chen/">C. Chen</a></span>, <span ><a href="/author/w.-bai/">W. Bai</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Medical Image Analysis, 83, 2023.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/mia-2023/">Generative myocardial motion tracking via latent space exploration with biomechanics-informed prior.</a>
  </h3>

  
  <a href="/publication/mia-2023/" class="summary-link">
    <div class="article-style">
      <p>Myocardial motion and deformation are rich descriptors that characterize cardiac function. Image registration, as the most commonly used technique for myocardial motion tracking, is an ill-posed inverse problem which often requires prior assumptions on the solution space. In contrast to most existing approaches which impose explicit generic regularization such as smoothness, in this work we propose a novel method that can implicitly learn an application-specific biomechanics-informed prior and embed it into a neural network-parameterized transformation model. Particularly, the proposed method leverages a variational autoencoder-based generative model to learn a manifold for biomechanically plausible deformations. The motion tracking then can be performed via traversing the learnt manifold to search for the optimal transformations while considering the sequence information. The proposed method is validated on three public cardiac cine MRI datasets with comprehensive evaluations. The results demonstrate that the proposed method can outperform other approaches, yielding higher motion tracking accuracy with reasonable volume preservation and better generalizability to varying data distributions. It also enables better estimates of myocardial strains, which indicates the potential of the method in characterizing spatiotemporal signatures for understanding cardiovascular diseases.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1016/j.media.2022.102682" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/c.-ouyang/">C. Ouyang</a></span>, <span ><a href="/author/c.-biffi/">C. Biffi</a></span>, <span ><a href="/author/c.-chen/">C. Chen</a></span>, <span ><a href="/author/t.-kart/">T. Kart</a></span>, <span ><a href="/author/h.-qiu/">H. Qiu</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE Transactions on Medical Imaging, 41(7):1837-1848, 2022.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/tmi-2022/">Self-supervised Learning for Few-shot Medical Image Segmentation.</a>
  </h3>

  
  <a href="/publication/tmi-2022/" class="summary-link">
    <div class="article-style">
      <p>Fully-supervised deep learning segmentation models are inflexible when encountering new unseen semantic classes and their fine-tuning often requires significant amounts of annotated data. Few-shot semantic segmentation (FSS) aims to solve this inflexibility by learning to segment an arbitrary unseen semantically meaningful class by referring to only a few labeled examples, without involving fine-tuning. State-of-the-art FSS methods are typically designed for segmenting natural images and rely on abundant annotated data of training classes to learn image representations that generalize well to unseen testing classes. However, such a training mechanism is impractical in annotation-scarce medical imaging scenarios. To address this challenge, in this work, we propose a novel self-supervised FSS framework for medical images, named SSL-ALPNet, in order to bypass the requirement for annotations during training. The proposed method exploits superpixel-based pseudo-labels to provide supervision signals. In addition, we propose a simple yet effective adaptive local prototype pooling module which is plugged into the prototype networks to further boost segmentation accuracy. We demonstrate the general applicability of the proposed approach using three different tasks: organ segmentation of abdominal CT and MRI images respectively, and cardiac segmentation of MRI images. The proposed method yields higher Dice scores than conventional FSS methods which require manual annotations for training in our experiments.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/TMI.2022.3150682" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/qt.-t.-mueller/">QT. T. Mueller</a></span>, <span ><a href="/author/j.-c.-paetzold/">J. C. Paetzold</a></span>, <span ><a href="/author/c.-prabhakar/">C. Prabhakar</a></span>, <span ><a href="/author/d.-usynin/">D. Usynin</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/g.-kaissis/">G. Kaissis</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE transactions on medical imaging, 43(4): 1489-1500, 2024
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/tpami-2023/">Private Graph Neural Networks for Whole-Graph Classification.</a>
  </h3>

  
  <a href="/publication/tpami-2023/" class="summary-link">
    <div class="article-style">
      <p>Graph Neural Networks (GNNs) have established themselves as state-of-the-art for many machine learning applications such as the analysis of social and medical networks. Several among these datasets contain privacy-sensitive data. Machine learning with differential privacy is a promising technique to allow deriving insight from sensitive data while offering formal guarantees of privacy protection. However, the differentially private training of GNNs has so far remained under-explored due to the challenges presented by the intrinsic structural connectivity of graphs. In this work, we introduce a framework for differential private graph-level classification. Our method is applicable to graph deep learning on multi-graph datasets and relies on differentially private stochastic gradient descent (DP-SGD). We show results on a variety of datasets and evaluate the impact of different GNN architectures and training hyperparameters on model performance for differentially private graph classification, as well as the scalability of the method on a large medical dataset. Our experiments show that DP-SGD can be applied to graph classification tasks with reasonable utility losses. Furthermore, we apply explainability techniques to assess whether similar representations are learned in the private and non-private settings. Our results can also function as robust baselines for future work in this area.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/TPAMI.2022.3228315" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/a.-d.-edwards/">A. D. Edwards</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/s.-m-smith/">S. M Smith</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Frontiers in Neuroscience, 16, 2022.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/frontiers-2022/">The Developing Human Connectome Project Neonatal Data Release.</a>
  </h3>

  
  <a href="/publication/frontiers-2022/" class="summary-link">
    <div class="article-style">
      <p>The Developing Human Connectome Project has created a large open science resource which provides researchers with data for investigating typical and atypical brain development across the perinatal period. It has collected 1228 multimodal magnetic resonance imaging (MRI) brain datasets from 1173 fetal and/or neonatal participants, together with collateral demographic, clinical, family, neurocognitive and genomic data from 1173 participants, together with collateral demographic, clinical, family, neurocognitive and genomic data. All subjects were studied in utero and/or soon after birth on a single MRI scanner using specially developed scanning sequences which included novel motion-tolerant imaging methods. Imaging data are complemented by rich demographic, clinical, neurodevelopmental, and genomic information. The project is now releasing a large set of neonatal data; fetal data will be described and released separately. This release includes scans from 783 infants of whom: 583 were healthy infants born at term; as well as preterm infants; and infants at high risk of atypical neurocognitive development. Many infants were imaged more than once to provide longitudinal data, and the total number of datasets being released is 887. We now describe the dHCP image acquisition and processing protocols, summarize the available imaging and collateral data, and provide information on how the data can be accessed.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.3389/fnins.2022.886772" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/d.-usynin/">D. Usynin</a></span>, <span ><a href="/author/a.-ziller/">A. Ziller</a></span>, <span ><a href="/author/m.-makowski/">M. Makowski</a></span>, <span ><a href="/author/r.-braren/">R. Braren</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/b.-glocker/">B. Glocker</a></span>, <span ><a href="/author/g.-kaissis/">G. Kaissis</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Nature Machine Intelligence (3), 749–758 2021.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/nmi-2021/">Adversarial interference and its mitigations in privacy-preserving collaborative machine learning.</a>
  </h3>

  
  <a href="/publication/nmi-2021/" class="summary-link">
    <div class="article-style">
      <p>Despite the rapid increase of data available to train machine-learning algorithms in many domains, several applications suffer from a paucity of representative and diverse data. The medical and financial sectors are, for example, constrained by legal, ethical, regulatory and privacy concerns preventing data sharing between institutions. Collaborative learning systems, such as federated learning, are designed to circumvent such restrictions and provide a privacy-preserving alternative by eschewing data sharing and relying instead on the distributed remote execution of algorithms. However, such systems are susceptible to malicious adversarial interference attempting to undermine their utility or divulge confidential information. Here we present an overview and analysis of current adversarial attacks and their mitigations in the context of collaborative machine learning. We discuss the applicability of attack vectors to specific learning contexts and attempt to formulate a generic foundation for adversarial influence and mitigation mechanisms. We moreover show that a number of context-specific learning conditions are exploited in similar fashion across all settings. Lastly, we provide a focused perspective on open challenges and promising areas of future research in the field.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1038/s42256-021-00390-3" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/s.-k.-zhou/">S. K. Zhou</a></span>, <span ><a href="/author/h.-greenspan/">H. Greenspan</a></span>, <span ><a href="/author/c.-davatzikos/">C. Davatzikos</a></span>, <span ><a href="/author/j.-s.-duncan/">J. S. Duncan</a></span>, <span ><a href="/author/b.-van-ginneken/">B. van Ginneken</a></span>, <span ><a href="/author/a.-madabhushi/">A. Madabhushi</a></span>, <span ><a href="/author/j.-l.-prince/">J. L. Prince</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/r.-m.-summers/">R. M. Summers</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Proceedings of the IEEE, 2021.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ieee-2021/">A Review of Deep Learning in Medical Imaging: Imaging Traits, Technology Trends, Case Studies with Progress Highlights, and Future Promises.</a>
  </h3>

  
  <a href="/publication/ieee-2021/" class="summary-link">
    <div class="article-style">
      <p>Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first present traits of medical imaging, highlight both clinical needs and technical challenges in medical imaging, and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1109/jproc.2021.3054390" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/q.-dou/">Q. Dou</a></span>, <span ><a href="/author/t.-y.-so/">T. Y. So</a></span>, <span ><a href="/author/m.-jiang/">M. Jiang</a></span>, <span ><a href="/author/q.-liu/">Q. Liu</a></span>, <span ><a href="/author/v.-vardhanabhuti/">V. Vardhanabhuti</a></span>, <span ><a href="/author/g.-kaissis/">G. Kaissis</a></span>, <span ><a href="/author/z.-li/">Z. Li</a></span>, <span ><a href="/author/w.-si/">W. Si</a></span>, <span ><a href="/author/h.-h.-c.-lee/">H. H. C. Lee</a></span>, <span ><a href="/author/k.-yu/">K. Yu</a></span>, <span ><a href="/author/z.-feng/">Z. Feng</a></span>, <span ><a href="/author/l.-dong/">L. Dong</a></span>, <span ><a href="/author/e.-burian/">E. Burian</a></span>, <span ><a href="/author/f.-jungmann/">F. Jungmann</a></span>, <span ><a href="/author/r.-braren/">R. Braren</a></span>, <span ><a href="/author/m.-makowski/">M. Makowski</a></span>, <span ><a href="/author/b.-kainz/">B. Kainz</a></span>, <span ><a href="/author/d.-rueckert/">D. Rueckert</a></span>, <span ><a href="/author/b.-glocker/">B. Glocker</a></span>, <span ><a href="/author/s.-c.-h.-yu/">S. C. H. Yu</a></span>, <span ><a href="/author/p.-a.-heng/">P. A. Heng</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      NPJ Digital Medicine 60(4), 2021.
    
  </span>
  

  

  
  
  
  
  

  
  

</div>

  

  
  
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/dm-2021/">Federated deep learning for detecting COVID-19 lung abnormalities in CT: A privacy-preserving multinational validation study.</a>
  </h3>

  
  <a href="/publication/dm-2021/" class="summary-link">
    <div class="article-style">
      <p>Data privacy mechanisms are essential for rapidly scaling medical training databases to capture the heterogeneity of patient data distributions toward robust and generalizable machine learning systems. In the current COVID-19 pandemic, a major focus of artificial intelligence (AI) is interpreting chest CT, which can be readily used in the assessment and management of the disease. This paper demonstrates the feasibility of a federated learning method for detecting COVID-19 related CT abnormalities with external validation on patients from a multinational study. We recruited 132 patients from seven multinational different centers, with three internal hospitals from Hong Kong for training and testing, and four external, independent datasets from Mainland China and Germany, for validating model generalizability. We also conducted case studies on longitudinal scans for automated estimation of lesion burden for hospitalized COVID-19 patients. We explore the federated learning algorithms to develop a privacy-preserving AI model for COVID-19 medical image diagnosis with good generalization capability on unseen multinational datasets. Federated learning could provide an effective mechanism during pandemics to rapidly develop clinically useful AI across institutions and countries overcoming the burden of central aggregation of large amounts of sensitive data.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  

















<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/https://doi.org/10.1038/s41746-021-00431-6" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Impressum</a>
    
    
  </p>
  

  <p class="powered-by">
    © Technical University of Munich 2024
  </p>

  
  






  <p class="powered-by">
    
    
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/wowchemy.min.d9d80f811e95b4b4f6df1eaaf297b05f.js"></script>

    






</body>
</html>
